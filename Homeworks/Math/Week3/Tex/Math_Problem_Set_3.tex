\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{\footnotesize\textsl{OSM Lab, Summer 2017, Math PS \#3}}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
%\numberwithin{equation}{section}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}

\begin{document}

\begin{flushleft}
   \textbf{\large{Math Homework Week \#3, Spectral Theory}} \\[5pt]
   OSM Lab, Eun-Seok Lee \\[5pt]

\end{flushleft}

\vspace{5mm}

\begin{enumerate}

	\item (4.2) 

$p = ax^2 + bx + c \\ \\
D(p)(x)=2ax+b\\ \\
 \therefore \begin{bmatrix}
0&1&0\\0&0&2\\0&0&0\end{bmatrix} \begin{bmatrix}
c\\b\\a\end{bmatrix} = \begin{bmatrix}b\\2a\\0\end{bmatrix}$  \\ \\ \\
$adjoint = \begin{bmatrix}
0&1&0\\0&0&2\\0&0&0\end{bmatrix}^T$ \\

$det\begin{bmatrix} - \lambda & 1 & 0\\ 0 & -\lambda & 2\\ 0 & 0 &-\lambda\end{bmatrix} = -\lambda^3=0 \\ \\ \therefore \lambda = 0$ \\
So, algebraic multiplicities: 3 \\
geometric multiplicities: 3 - 2(rank) = 1 


	\item (4.4) \\
$(i) A=\begin{bmatrix} a&b \\c&d\end{bmatrix} A^H=\begin{bmatrix}\bar{a} & \bar{b} \\ \bar{c} &\bar{d} \end{bmatrix}$ \\
$\lambda-(a+d)\lambda+(ad-bc)=0\\
b=\bar{c}, c=\bar{b}, a=\bar{a}, d=\bar{d}\\
\lambda^2-(a+d)\lambda+(ad-b^2)=0\\
(a-d)^2+4c\bar{c} \geq 0 \\ \\
(ii) A=\begin{bmatrix} a&b \\c&d\end{bmatrix} A^H=\begin{bmatrix}\bar{a} & \bar{b} \\ \bar{c} &\bar{d} \end{bmatrix}\\
b=-\bar{c}, c=-\bar{b}, a=-\bar{a}, d=-\bar{d}\\
(a-d)^2 -4b\bar{b} < 0 \ (\because a, d = 0$ or imaginary numbers.$)$

	\item (4.6) \\
Let diagonal entries of an upper-triangular matrix $A$ be $d_1, d_2, d_3, \cdots, d_n$. Then, $(A-\lambda I)$ is also upper triangular matrix. Now, diagonal entries are $d_i-\lambda$. $|\pi_i(d_i-\lambda)| = 0$ and $\lambda$ is $d_i$ here. For lower triangular matrix, the proof is almost same.


	\item (4.8) \\
(i)
I think that it has to be $C^{\infty}([-\pi,\pi],R)$ such as Exercise 3.8. I think it might be typo. If it is $C^{\infty}([-\pi,\pi],R)$, then I can prove as the following. \\ \\



(1)
$\frac{1}{\pi}\int_{-\pi}^{\pi} cost \cdot sint \ dt $ = $\frac{1}{\pi}\int_{-\pi}^{\pi} \frac{1}{2} sin2t \ dt $ 
\\
\\
$=\frac{1}{2\pi}[-\frac{1}{2} cos2t]_{-\pi}^{pi}$
\\
$=0$
\\
\\
(2)
$\frac{1}{\pi}\int_{-\pi}^{\pi} cost \cdot cos2t \ dt $ = $\frac{1}{\pi}\int_{-\pi}^{\pi} cost (2cos^2t-1) \ dt $
\\
\\
$=\frac{1}{\pi}\int_{-\pi}^{\pi} (2cos^3t-cost) \ dt $
\\
\\
$=\frac{1}{\pi}\int_{-\pi}^{\pi} \frac{1}{2}(cos3t+cost) \ dt $
\\
\\
$=\frac{1}{\pi}[\frac{1}{6}(sin3t)+\frac{1}{2}sint]_{-\pi}^{\pi} $
\\
\\
$=0$
\\
\\
(3)
$\frac{1}{\pi}\int_{-\pi}^{\pi} cost \cdot sin2t \ dt $ 
$=\frac{1}{\pi}\int_{-\pi}^{\pi} 2cos^2t \cdot sint \ dt $ 
\\
\\
$=\frac{1}{\pi}\int_{-\pi}^{\pi} 2(sint - sin^3t) \ dt $ 
\\
\\
$=\frac{1}{\pi}[\frac{1}{2}(-cost)+\frac{1}{6}(-cos3t)]_{-\pi}^{\pi} $ 
$=0$
\\
\\
(4)
$\int_{-\pi}^{\pi} sint \cdot cos2t \ dt $ 
$=\int_{-\pi}^{\pi} sint(1-2sin^2t) \ dt $ 
\\
\\
$=\int_{-\pi}^{\pi} -\frac{1}{2}sint + \frac{3}{2}sin3t \ dt $ 
\\
\\
$=[\frac{1}{2}cost - \frac{1}{2}cost3t]_{-\pi}^{\pi} $
\\
\\
$=0$
\\
\\
(5)
$\int_{-\pi}^{\pi} sint \cdot sin2t \ dt $ 
$=\int_{-\pi}^{\pi} 2sin^2t cost \ dt $ 
$=\int_{-\pi}^{\pi} 2(1-cos^2t)cost \ dt $ 
\\
\\
$=\int_{-\pi}^{\pi} 2cost - 2 \cdot \frac{1}{4}(3cost + cos3t) \ dt $ 
$=\int_{-\pi}^{\pi} \frac{1}{2}cost - \frac{1}{2}cos3t \ dt $ 
\\
\\
$=[\frac{1}{2}sint-\frac{1}{6}sin3t]_{-\pi}^{\pi}$
\\
\\
$=0$
\\
\\
(6)
$\int_{-\pi}^{\pi} cos2t \cdot sin2t \ dt $
\\
\\
$\int_{-2\pi}^{2\pi} cost \cdot sint (\frac{1}{2}) \ dt $
$=0$
\\
\\
Thus, S is orthogonal set. By definition, it is linear independent.
\\

(ii)
$D=\begin{bmatrix} 0&1&0&0 \\ -1&0&0&0 \\ 0&0&0&2 \\ 0&0&-2&0 \end{bmatrix}\begin{bmatrix} sinx \\ cosx \\sin2x \\cos2x\end{bmatrix}$
\\
\\
(iii)
$span[(1,0,0,0),(0,1,0,0)]$ and $span[(0,0,1,0), (0,0,0,1)]$

	\item (4. 13) \\
$P = \begin{bmatrix} 1&1 \\ 0.5&-1 \end{bmatrix}\\
Then, P^{-1}AP=\frac{1}{detp} \begin{bmatrix} -15&0 \\ 0&-6 \end{bmatrix}\\$
I calculated $P^{-1}AP$ by general $P=\begin{bmatrix} a&b \\ c&d \end{bmatrix}$, then input numbers.



	\item (4.15) \\
$A=P^{-1}BP$, then $A^k= P^{-1}B^kP\\$
If A is semisimple, then it is diagonalizable. By Thm 4.3.7 proof, $A=P^{-1}DP where D=diag(\lambda_1,\lambda_2, \cdots, \lambda_n)\\$
Then, 
$A^n= P^{-1}D^nP$, so $D^2=diag(\lambda_1^{n}, \cdots, \lambda_n^{n})$\\
Then, $(a_0+a_1A+ \cdots +a_nA^n)x=(a_0+\lambda + \lambda^2 + \cdots + \lambda^n)x\\$
So, $f(A)x = f(\lambda)x$





	\item (4.16) \\
(i) $\lim_{n\rightarrow\infty} A^n $ with respect to the 1-norm. \\
$P^{-1}AP=D$ where $P=\begin{bmatrix} 2&1 \\ 1&-2 \end{bmatrix}$ from eigen vectors.
So, we can easily calculate the answer $\begin{bmatrix} 2/3&2/3 \\ 1/3& 1/3\end{bmatrix}$ \\

(ii) In any form of norms, it has to be same in a reason that we calculate before taking norm.\\ \\
(iii) $f(x) = 3+5x+x^3 \\ \therefore f(\lambda_1)=9$ and $f(\lambda_2)=5.064$




	\item (4.18) \\
From $(Ax)=(\lambda x)$, $(Ax)^T=x^TA=(\lambda x)^T = x^T\lambda = \lambda x^T$




	\item (4.20) \\
If A is Hermitian and orthonormally similar to B, then, B is also Hermitian.\\
$\Rightarrow B=U^HAU$ where $U$ is orthonormal. \\$ B^H =U^HA^HU=U^HAU=B$ by $A$ is Hermitian.





	\item (4.24) \\
(i) $A=A^H \
\rho(x) = \frac{<x,Ax>}{\|x\|_2}=\frac{x^HAx}{\|x\|_2} = \frac{<x,\lambda x>}{\|x\|_2}=\frac{x^H\lambda x}{\|x\|_2} =\lambda = \frac{<A^Hx,x>}{\|x\|_2}=\frac{\bar{\lambda}x^Hx}{\|x\|_2} = \bar{\lambda}\\
A=-A^H \\$
$\rho(x) = \frac{<x,Ax>}{\|x\|_2}=\frac{<x,\lambda x>}{\| x\|_2}=\frac{\lambda x^Hx}{\| x\|_2} = \lambda = \frac{<A^Hx, x>}{\| x\|_2}=-\frac{<Ax, x>}{\|x\|_2}=-\frac{\bar{\lambda}x^Hx}{\|x\|_2}=-\bar{\lambda}
$



	\item (4.25) \\
(i) $(x_1x_1^H + \cdots +x_nx_n^H)(a_1x_1 + a_2x_2 + \cdots + a_nx_n) = (a_1x_1 + \cdots + a_nb_n)\\
\because x_j^Hx_j = 1 $  and by orthogonality.\\
By the way, $ (a_1x_1 + \cdots + a_nb_n)$ is arbitrary linear combination, so, \\$(x_1x_1^H + \cdots +x_nx_n^H)$ has to be $I$.
\\
\\
(ii) $AI = A(x_1x_1^H + \cdots +x_nx_n^H)\\
\therefore A = Ax_1x_1^H + \cdots +Ax_nx_n^H = \lambda x_1x_1^H + \cdots + \lambda x_nx_n^H
$

	\item (4.27) \\
let $x_i=(0, \cdots, 1+a*i, \cdots, 0)$.\\
Then, $x_i^HAx_I=((1+a*i)^Ha_{ii}(1+a*i)>0\\
\therefore a_{ii}>0 \\
(\because A$ is hermitian, so $A=A^H)$\\
Therefore, $a_{ii}$ is real.



 	\item (4.28) \\
We already know that trace is one of inner product in Chapter 3. So, it has to satisfy Cauchy-Schwartz ineqaulity.\\
So, $ tr(AB) \leq \sqrt{tr(A^2)tr(B^2)} = \sqrt{\sum (\lambda_i^A)^2\sum (\lambda_i^B)^2} \leq \sqrt{(\sum \lambda_i^A)^2(\sum \lambda_i^B)^2} = tr(A)tr(B)
$



 	\item (4.31) \\
(i) $\|A\|_2 = sup\frac{\| Ax\|_2}{\| x\|_2} =sup\frac{\| U\Sigma V^Hx\|_2}{\| x\|_2} =sup\frac{\| \Sigma V^Hx\|_2}{\| x\|_2} =sup\frac{\| \Sigma V^Hx\|_2}{\| V^Hx\|_2} = sup\frac{(\sum|\sigma_i V^Hx|^2 )^{0.5}}{(\sum|V^Hx|^2 )^{0.5}} \\$
(ii) By the method of (i), now $\frac{1}{\sigma_n}=\|A^{-1}\|_2 $ (It is the largest here.)$
$
(iii) $\| A^H\|_2^2=\sigma_1^2=\| A^T\|_2 = \sigma_1^2 = \| A^HA\|_2 = \sigma_1^2 = (\| A\|_2)^2\\$
(iv) $\| UAV\|_2 = \| UU\Sigma V^HV\|_2 = \| \Sigma\|_2 = \| A\|_2$





 	\item (4.32) \\
(i) $\| UAV\|_F =  \| AV\|_F = \| U\Sigma V^HV\|_F = \| \Sigma \|_F = \| V \Sigma \|_F = \| \Sigma^H V^H \|_F \\(\because $ trace property)\\
$= \| U \Sigma V^H\|_F = \| A \|_F
$\\
(ii) $\| \Sigma \|_F = \sqrt{tr(\Sigma^H\Sigma)} = \sqrt{\sigma_1^2 + \cdots + \sigma_n^2}$



 	\item (4.33) \\
$\| A \|_2 = \| \Sigma \|_2 = sup|y^H\Sigma x| = \sigma_1
$\\ I already showed $\| A \|_2 = \| \Sigma \|_2 $  above.


 	\item (4.36) \\
$A = \begin{bmatrix} 1&2\\0&3  \end{bmatrix}\\
\lambda^2-4\lambda+3=0 \Rightarrow \lambda = 1$ or $3.$
From $A^HA=\begin{bmatrix} 1&0\\2&3  \end{bmatrix} \begin{bmatrix} 1&2\\0&3  \end{bmatrix}$, \\$\sigma_1 \approx 3.6503$, $\sigma_2\approx 0.8219
$




 	\item (4.38) \\
(i) $AA^+A = U_1\Sigma_1V_1^HV_1\Sigma_1^{-1}U_1^HA = U_1U_1^HA = U_1U_1^HU_1\Sigma_1V_1^H = U_1\Sigma_1V_1^H =A \\$
(ii) $A^+AA^+=V_1\Sigma^{-1}U_1^H = A^+\\$
(iii) $(AA^+)^H=U_1U_1^H=AA^+$ from (i) we can easily know it.\\
(iv) $ (A^+A)^H = V_1V_1^H = V_1\Sigma^{-1}U_1^HU_1\Sigma_1V_1^H = A^+A $\\
(v) From (iii), it has to be real.\\
From orthogonal projection, $A(A^HA)^{-1}A^H$, we can know that $A(A^HA)^{-1}A^HA = AA^+A = A$ \\
(vi) From (iv), it has to be real.\\
From orthogonal projection, $A^H(AA^H)^{-1}A$, we can know that $AA^H(AA^H)^{-1}A = AA^+A = A\\
$






















\end{enumerate}

\vspace{25mm}

\bibliography{ProbStat_probset}

\end{document}
